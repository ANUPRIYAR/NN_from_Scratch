{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "II5cON_gL1CV"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hLKeTKXQzd3z"
   },
   "source": [
    "### Build a simple Neural Network on SVHN dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_eDZnLdazd31"
   },
   "source": [
    "#### Import the data from drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uKldcB6Rzd33",
    "outputId": "fcbe07d5-2bea-4148-9bad-9d1923ef8edd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "wybzdxckzd3_",
    "outputId": "ebc36bc5-f023-4a3c-b3eb-1abef5fbcbc0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (42000, 1024) (42000,)\n",
      "Test set (18000, 1024) (18000,)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# Open the file as readonly\n",
    "h5f = h5py.File('/content/drive/My Drive/DLCP/Project-1/Data/SVHN_single_grey1.h5', 'r')\n",
    "\n",
    "# Load the training, test and validation set\n",
    "x_train = h5f['X_train'][:]\n",
    "y_train = h5f['y_train'][:]\n",
    "x_test = h5f['X_test'][:]\n",
    "y_test = h5f['y_test'][:]\n",
    "\n",
    "\n",
    "# Close this file\n",
    "h5f.close()\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 1024)\n",
    "x_test = x_test.reshape(x_test.shape[0], 1024)\n",
    "\n",
    "# # normalize inputs from 0-255 to 0-1\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "print('Training set', x_train.shape, y_train.shape)\n",
    "print('Test set', x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "uh2EKuGPzd4J",
    "outputId": "ab6c70fd-fc14-4b84-c3c1-a7dc52816338"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 1024)\n",
      "(42000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "QikBBAOMzd4S",
    "outputId": "895a1c89-7c22-46a1-d897-510debc95d21",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18000, 1024)\n",
      "(18000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2K1_j5KLzd4b"
   },
   "source": [
    "### Neural network from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Uk0QZP-zd4d"
   },
   "source": [
    "\n",
    "#### Fully Connected Layer (Linear Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xI8m3r0Wzd4e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Linear():\n",
    "    def __init__(self, rows, cols):\n",
    "        self.W = np.random.randn(rows, cols) * 0.01\n",
    "        self.b = np.zeros((1, cols))\n",
    "        self.params = [self.W, self.b]\n",
    "        self.gradW = None\n",
    "        self.gradB = None\n",
    "        self.gradInput = None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.W) + self.b\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, next_grad):\n",
    "        self.gradW = np.dot(self.inputs.T, next_grad)\n",
    "        self.gradB = np.sum(next_grad, axis=0)\n",
    "        self.gradInput = np.dot(next_grad, self.W.T)\n",
    "        return self.gradInput, [self.gradW, self.gradB]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EB36aapbzd4l"
   },
   "source": [
    "#### Rectified Linear Activation Layer (ReLU)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Idwz_ehfWPiF"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o1TzFMADzd4n"
   },
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.gradInput = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.output = np.maximum(X, 0)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self,  next_grad):\n",
    "        self.gradInput =  next_grad.copy()\n",
    "        self.gradInput[self.output <=0] = 0\n",
    "        return self.gradInput, []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HMA6A6C3zd4s"
   },
   "source": [
    "#### Define softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hNSXYcTUzd4w"
   },
   "outputs": [],
   "source": [
    "def softmax(inp):\n",
    "    deno = np.sum(np.exp(inp),axis =1 , keepdims=True)\n",
    "    return np.exp(inp)/ deno  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IniE9r0zd46"
   },
   "source": [
    "#### Define the Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s39F0zr1zd49"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CrossEntropy():\n",
    "    def forward(self, X, y):\n",
    "        self.m = y.shape[0]\n",
    "        self.p = softmax(X)\n",
    "        cross_entropy = -np.log(self.p[range(self.m), y]+1e-16)\n",
    "        loss = np.sum(cross_entropy) / self.m\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        y_idx = y.argmax()        \n",
    "        grad = softmax(X)\n",
    "        grad[range(self.m), y] -= 1\n",
    "        grad /= self.m\n",
    "        return grad\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eHVJSau3zd5D"
   },
   "source": [
    "#### Define the container NN class that enables the forward prop and backward propagation of the entire network. Note, how this class enables us to add layers of different types and also correctly pass gradients using the chain rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "49_stYd9zd5I"
   },
   "outputs": [],
   "source": [
    "class NN():\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.layers = []\n",
    "        self.grads = []\n",
    "        self.loss_obj = CrossEntropy()\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        self.params.append(layer.params)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        for layer in self.layers:\n",
    "            inp = layer.forward(inp)\n",
    "        return inp\n",
    "\n",
    "    def backward(self, next_grad):\n",
    "        self.clear_grads()\n",
    "        for layer in reversed(self.layers):\n",
    "            next_grad, grad = layer.backward(next_grad)\n",
    "            self.grads.append(grad)\n",
    "        return self.grads\n",
    "\n",
    "    def train_step(self, x, y):\n",
    "        out = self.forward(x)\n",
    "        loss = self.loss_obj.forward(out, y)\n",
    "        next_grad = self.loss_obj.backward(out, y)\n",
    "        grads = self.backward(next_grad)\n",
    "        return loss, grads\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = self.forward(x)\n",
    "        p = softmax(x)\n",
    "        return np.argmax(p, axis=1)\n",
    "\n",
    "    def clear_grads(self):\n",
    "        self.grads = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y_QJglRNzd5T"
   },
   "source": [
    "#### Define the update function (SGD with momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RSmr4zmkzd5W"
   },
   "outputs": [],
   "source": [
    "def update_params(velocity, params, grads, learning_rate=0.01, mu=0.9):\n",
    "    for v, p, g, in zip(velocity, params, reversed(grads)):\n",
    "        for i in range(len(g)):\n",
    "            v[i] = mu * v[i] - learning_rate * g[i]\n",
    "            p[i] += v[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uSLyB5v3zd5c"
   },
   "source": [
    "#### Define a function which gives us the minibatches (both the datapoint and the corresponding label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VhTHzovHzd5e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import shuffle\n",
    "\n",
    "def minibatches(x_train,y_train,batchsize):\n",
    "  minibatch =[]\n",
    "  \n",
    "  index_list = [i for i in range(x_train.shape[0])]\n",
    "  shuffle(index_list)  \n",
    "  \n",
    "  # Shuffle the entire training data set\n",
    "  x_tr = x_train[index_list, :]\n",
    "  y_tr = y_train[index_list,]\n",
    "  X_list = []\n",
    "  Y_list = []\n",
    "  i = 0\n",
    "  \n",
    " # Divide the entire training set into mini batches \n",
    "  while (len(X_list) < (x_train.shape[0]/batchsize)):\n",
    "    X,Y = x_tr[i:i+batchsize,], y_tr[i:i+batchsize,]\n",
    "    X_list.append(X)\n",
    "    Y_list.append(Y)\n",
    "    i = i + batchsize\n",
    "  return X_list, Y_list\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0IA6Opirzd5m"
   },
   "source": [
    "#### The traning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IKTb1r9Ozd5o"
   },
   "outputs": [],
   "source": [
    "def train(net, x_train, y_train, x_validation, y_validation, minibatch_size,\n",
    "          nEpochs,\n",
    "          learning_rate, mu=0.9):\n",
    "    # Get the minibatches from the training set\n",
    "    X_batchlist, Y_batchlist = minibatches(x_train, y_train, batchsize=420)\n",
    "\n",
    "    # Training will be done 'nEpochs' times\n",
    "    for n in range(nEpochs):\n",
    "        loss_batch = []\n",
    "        val_loss_batch = []\n",
    "        velocity = []\n",
    "        for param_layer in net.params:\n",
    "            p = [np.zeros_like(param) for param in list(param_layer)]\n",
    "            velocity.append(p)\n",
    "\n",
    "        # Train the minibatch of training set and update the weight and bais\n",
    "        # parameters as per stochastic gradient descent algorithm\n",
    "        for x_batch, y_batch in zip(X_batchlist, Y_batchlist):\n",
    "            loss, grads = net.train_step(x_batch, y_batch)\n",
    "            loss_batch.append(loss)\n",
    "            update_params(velocity, net.params, grads,\n",
    "                          learning_rate=learning_rate, mu=mu)\n",
    "\n",
    "        # Get the minibatches of Test set for validation\n",
    "        x_val_list, y_val_list = minibatches(x_validation, y_validation,\n",
    "                                             batchsize=420)\n",
    "\n",
    "        # Get the average loss for validation set\n",
    "        for x_val_batch, y_val_batch in zip(x_val_list, y_val_list):\n",
    "            val_loss, val_grads = net.train_step(x_val_batch, y_val_batch)\n",
    "            val_loss_batch.append(val_loss)\n",
    "\n",
    "        avg_loss = sum(loss_batch) / float(len(loss_batch))\n",
    "        avg_val_loss = sum(val_loss_batch) / float(len(val_loss_batch))\n",
    "\n",
    "        # Initilaize prediction set and target set as numpy array for easier\n",
    "        # data manipulation\n",
    "        pred_val = np.array([], dtype='int64')\n",
    "        target_val = np.array([], dtype='int64')\n",
    "        accuracy_val = np.array([])\n",
    "        for i in range(0, 100):\n",
    "            prediction = net.predict(x_test[i])\n",
    "            pred_val = np.append(pred_val, prediction)\n",
    "            target_val = np.append(target_val, y_test[i])\n",
    "        percent_acc = check_accuracy(target_val, pred_val)\n",
    "        accuracy_val = np.append(accuracy_val, percent_acc)\n",
    "        \n",
    "\n",
    "        print(\"Train_Loss: {0} ; Val_loss:{1} ; percent_acc : {2} % \".format(\n",
    "            avg_loss, avg_val_loss, percent_acc))\n",
    "    avg_acc  = np.mean(accuracy_val)\n",
    "    print(\"Average accuracy acheived: {} %\".format(avg_acc))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iW1Se6qtzd5w"
   },
   "source": [
    "#### Write a function to check the accuracy of the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6A22MDvGzd5x"
   },
   "outputs": [],
   "source": [
    "def check_accuracy(target, predicted):\n",
    "    mean_value = np.mean(predicted == target)\n",
    "    acc_percent = mean_value * 100\n",
    "    return acc_percent\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TJeS1rkLzd52"
   },
   "source": [
    "**Invoke all that we have created until now and build a 2 linear layer Neural Network with first activation function as ReLU and second as Softmax and loss function as Cross Entropy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1812
    },
    "colab_type": "code",
    "id": "L0moM_WBzd53",
    "outputId": "1cf223cf-183d-4c3d-fc18-ae94aa2fb9af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Loss: 2.302248944992992 ; Val_loss:2.300728724854931 ; percent_acc : 19.0 % \n",
      "Train_Loss: 2.2950872955037154 ; Val_loss:2.2801694950162865 ; percent_acc : 26.0 % \n",
      "Train_Loss: 2.2339333230126557 ; Val_loss:2.136263942391175 ; percent_acc : 33.0 % \n",
      "Train_Loss: 1.9812683471250745 ; Val_loss:1.7759346318688176 ; percent_acc : 37.0 % \n",
      "Train_Loss: 1.6470461432173493 ; Val_loss:1.49674198993068 ; percent_acc : 56.00000000000001 % \n",
      "Train_Loss: 1.4297813497475205 ; Val_loss:1.312619158635046 ; percent_acc : 56.99999999999999 % \n",
      "Train_Loss: 1.299585022577588 ; Val_loss:1.210432716938166 ; percent_acc : 64.0 % \n",
      "Train_Loss: 1.2115645640540411 ; Val_loss:1.2050075010477577 ; percent_acc : 59.0 % \n",
      "Train_Loss: 1.1615834900101183 ; Val_loss:1.1499138500880364 ; percent_acc : 66.0 % \n",
      "Train_Loss: 1.1133757105377213 ; Val_loss:1.1141839856126639 ; percent_acc : 70.0 % \n",
      "Train_Loss: 1.070790682670418 ; Val_loss:1.0827782434273345 ; percent_acc : 70.0 % \n",
      "Train_Loss: 1.0577058454241597 ; Val_loss:1.057308821440591 ; percent_acc : 69.0 % \n",
      "Train_Loss: 1.014529692585696 ; Val_loss:1.0390319137740096 ; percent_acc : 67.0 % \n",
      "Train_Loss: 0.9928307989357278 ; Val_loss:1.008836432121132 ; percent_acc : 67.0 % \n",
      "Train_Loss: 0.9647117721611571 ; Val_loss:0.9890697271866201 ; percent_acc : 69.0 % \n",
      "Train_Loss: 0.954442785749244 ; Val_loss:1.0035813302496834 ; percent_acc : 68.0 % \n",
      "Train_Loss: 0.9301152179035063 ; Val_loss:0.9513106361544945 ; percent_acc : 71.0 % \n",
      "Train_Loss: 0.9092803291988066 ; Val_loss:0.9225971576345582 ; percent_acc : 75.0 % \n",
      "Train_Loss: 0.8972721455861847 ; Val_loss:0.9229100534092324 ; percent_acc : 74.0 % \n",
      "Train_Loss: 0.882655253808095 ; Val_loss:0.914244636649387 ; percent_acc : 71.0 % \n",
      "Train_Loss: 0.8652570278128016 ; Val_loss:0.9063687851159419 ; percent_acc : 73.0 % \n",
      "Train_Loss: 0.8558319604343094 ; Val_loss:0.903536661476964 ; percent_acc : 76.0 % \n",
      "Train_Loss: 0.8303940168907576 ; Val_loss:0.8572031509159579 ; percent_acc : 75.0 % \n",
      "Train_Loss: 0.8093956556241123 ; Val_loss:0.8664440650287892 ; percent_acc : 77.0 % \n",
      "Train_Loss: 0.807587390627368 ; Val_loss:0.8752370181186621 ; percent_acc : 74.0 % \n",
      "Train_Loss: 0.7865695084043958 ; Val_loss:0.8490460344776953 ; percent_acc : 74.0 % \n",
      "Train_Loss: 0.7786540561636157 ; Val_loss:0.8788572912767054 ; percent_acc : 74.0 % \n",
      "Train_Loss: 0.7766649361315365 ; Val_loss:0.8547152792933512 ; percent_acc : 75.0 % \n",
      "Train_Loss: 0.7558498352654884 ; Val_loss:0.8182702636397822 ; percent_acc : 73.0 % \n",
      "Train_Loss: 0.747868628194622 ; Val_loss:0.8696648350687326 ; percent_acc : 72.0 % \n",
      "Train_Loss: 0.741851805546414 ; Val_loss:0.8381433721456745 ; percent_acc : 76.0 % \n",
      "Train_Loss: 0.7447669093726997 ; Val_loss:0.88137355647637 ; percent_acc : 74.0 % \n",
      "Train_Loss: 0.7363325456898768 ; Val_loss:0.9226576759023775 ; percent_acc : 68.0 % \n",
      "Train_Loss: 0.7388173215366026 ; Val_loss:0.8655655436268467 ; percent_acc : 78.0 % \n",
      "Train_Loss: 0.7271384776394298 ; Val_loss:0.8949504981510155 ; percent_acc : 75.0 % \n",
      "Train_Loss: 0.7156229020968341 ; Val_loss:0.8594431879760873 ; percent_acc : 77.0 % \n",
      "Train_Loss: 0.7029537635274486 ; Val_loss:0.8480339725008292 ; percent_acc : 77.0 % \n",
      "Train_Loss: 0.692650431625975 ; Val_loss:0.8203668768079548 ; percent_acc : 74.0 % \n",
      "Train_Loss: 0.6819903661849744 ; Val_loss:0.8086826943411082 ; percent_acc : 74.0 % \n",
      "Train_Loss: 0.6835734226430809 ; Val_loss:0.829227248984722 ; percent_acc : 76.0 % \n",
      "Train_Loss: 0.6872667596177894 ; Val_loss:0.7999602907008886 ; percent_acc : 79.0 % \n",
      "Train_Loss: 0.6738310168650631 ; Val_loss:0.8536720268426717 ; percent_acc : 75.0 % \n",
      "Train_Loss: 0.668393145971683 ; Val_loss:0.836180948961137 ; percent_acc : 77.0 % \n",
      "Train_Loss: 0.6537718752846196 ; Val_loss:0.7894609562227903 ; percent_acc : 83.0 % \n",
      "Train_Loss: 0.6538512258169105 ; Val_loss:0.8171579467149749 ; percent_acc : 79.0 % \n",
      "Train_Loss: 0.6451493150245637 ; Val_loss:0.7683003497596607 ; percent_acc : 79.0 % \n",
      "Train_Loss: 0.6361462238660333 ; Val_loss:0.8127035771838873 ; percent_acc : 80.0 % \n",
      "Train_Loss: 0.6439077967162571 ; Val_loss:0.8191118601931481 ; percent_acc : 77.0 % \n",
      "Train_Loss: 0.6517724911165581 ; Val_loss:0.7932113440276 ; percent_acc : 78.0 % \n",
      "Train_Loss: 0.6335706626033281 ; Val_loss:0.7984276431739505 ; percent_acc : 75.0 % \n",
      "Train_Loss: 0.6276465902355192 ; Val_loss:0.7791126810892952 ; percent_acc : 77.0 % \n",
      "Train_Loss: 0.6238115741175482 ; Val_loss:0.7594680124001468 ; percent_acc : 79.0 % \n",
      "Train_Loss: 0.6158758285296365 ; Val_loss:0.7790988405359707 ; percent_acc : 79.0 % \n",
      "Train_Loss: 0.615306958310727 ; Val_loss:0.7590333710574547 ; percent_acc : 77.0 % \n",
      "Train_Loss: 0.608276597495189 ; Val_loss:0.7604457150306921 ; percent_acc : 80.0 % \n",
      "Train_Loss: 0.6072765041039604 ; Val_loss:0.7645616588787987 ; percent_acc : 79.0 % \n",
      "Train_Loss: 0.6069475748779837 ; Val_loss:0.7881734912253093 ; percent_acc : 77.0 % \n",
      "Train_Loss: 0.6104368946895561 ; Val_loss:0.7587015145785347 ; percent_acc : 77.0 % \n",
      "Train_Loss: 0.5995562002351973 ; Val_loss:0.7778890048961765 ; percent_acc : 80.0 % \n",
      "Train_Loss: 0.5961666664172532 ; Val_loss:0.7534132900628238 ; percent_acc : 78.0 % \n",
      "Train_Loss: 0.5966371584174769 ; Val_loss:0.7515841003591647 ; percent_acc : 78.0 % \n",
      "Train_Loss: 0.5854568697341442 ; Val_loss:0.7260306694217517 ; percent_acc : 80.0 % \n",
      "Train_Loss: 0.5861632194415093 ; Val_loss:0.7428233746858736 ; percent_acc : 79.0 % \n",
      "Train_Loss: 0.5839370039190356 ; Val_loss:0.7347512015763746 ; percent_acc : 79.0 % \n",
      "Train_Loss: 0.5776909318826805 ; Val_loss:0.7357213853999549 ; percent_acc : 81.0 % \n",
      "Train_Loss: 0.5774561840251287 ; Val_loss:0.748178361356494 ; percent_acc : 79.0 % \n",
      "Train_Loss: 0.570376797804795 ; Val_loss:0.7473271197313908 ; percent_acc : 81.0 % \n",
      "Train_Loss: 0.5772222445273302 ; Val_loss:0.713499331158612 ; percent_acc : 82.0 % \n",
      "Train_Loss: 0.5686006369831763 ; Val_loss:0.7451914421440013 ; percent_acc : 81.0 % \n",
      "Train_Loss: 0.5688318751138857 ; Val_loss:0.7442232115562302 ; percent_acc : 82.0 % \n",
      "Train_Loss: 0.5618804988571721 ; Val_loss:0.7177279373044856 ; percent_acc : 78.0 % \n",
      "Train_Loss: 0.5570700907401603 ; Val_loss:0.7401470889362628 ; percent_acc : 82.0 % \n",
      "Train_Loss: 0.5552610922965734 ; Val_loss:0.7194500613207834 ; percent_acc : 80.0 % \n",
      "Train_Loss: 0.5410688533633564 ; Val_loss:0.7164942158749561 ; percent_acc : 83.0 % \n",
      "Train_Loss: 0.5428085669881341 ; Val_loss:0.7040057092225467 ; percent_acc : 81.0 % \n",
      "Train_Loss: 0.5392019430445452 ; Val_loss:0.7185487359900294 ; percent_acc : 81.0 % \n",
      "Train_Loss: 0.5349429619386646 ; Val_loss:0.7122010813540832 ; percent_acc : 81.0 % \n",
      "Train_Loss: 0.5289560028252436 ; Val_loss:0.7109185730889139 ; percent_acc : 79.0 % \n",
      "Train_Loss: 0.5361739073319306 ; Val_loss:0.7105727457418546 ; percent_acc : 82.0 % \n",
      "Train_Loss: 0.533879269018788 ; Val_loss:0.7204665567995092 ; percent_acc : 81.0 % \n",
      "Train_Loss: 0.5239078381213124 ; Val_loss:0.7078482008324474 ; percent_acc : 81.0 % \n",
      "Train_Loss: 0.5231341952189136 ; Val_loss:0.6947404735663036 ; percent_acc : 81.0 % \n",
      "Train_Loss: 0.5193982104752394 ; Val_loss:0.7122571612492663 ; percent_acc : 80.0 % \n",
      "Train_Loss: 0.5221407540437184 ; Val_loss:0.6884478343605408 ; percent_acc : 82.0 % \n",
      "Train_Loss: 0.515477496567133 ; Val_loss:0.6869251254738172 ; percent_acc : 80.0 % \n",
      "Train_Loss: 0.5198233558195504 ; Val_loss:0.7178643265341192 ; percent_acc : 79.0 % \n",
      "Train_Loss: 0.5157258357674004 ; Val_loss:0.6868179365151342 ; percent_acc : 84.0 % \n",
      "Train_Loss: 0.5054963404440147 ; Val_loss:0.6925390344298811 ; percent_acc : 83.0 % \n",
      "Train_Loss: 0.5184485682709202 ; Val_loss:0.7151002460807221 ; percent_acc : 78.0 % \n",
      "Train_Loss: 0.5212729424735976 ; Val_loss:0.7007474929195939 ; percent_acc : 83.0 % \n",
      "Train_Loss: 0.5059270647087571 ; Val_loss:0.6774377365208623 ; percent_acc : 81.0 % \n",
      "Train_Loss: 0.5068889023239478 ; Val_loss:0.7111691762314091 ; percent_acc : 82.0 % \n",
      "Train_Loss: 0.5135366087033453 ; Val_loss:0.708094750796938 ; percent_acc : 78.0 % \n",
      "Train_Loss: 0.5176611382548679 ; Val_loss:0.7067772050710204 ; percent_acc : 77.0 % \n",
      "Train_Loss: 0.5110467570453787 ; Val_loss:0.723641895786167 ; percent_acc : 79.0 % \n",
      "Train_Loss: 0.5074055534866669 ; Val_loss:0.7173150686939026 ; percent_acc : 82.0 % \n",
      "Train_Loss: 0.503285917523209 ; Val_loss:0.6972680456457805 ; percent_acc : 83.0 % \n",
      "Train_Loss: 0.5033872539220965 ; Val_loss:0.707237744745674 ; percent_acc : 77.0 % \n",
      "Train_Loss: 0.5002841255773226 ; Val_loss:0.7015767273401367 ; percent_acc : 83.0 % \n",
      "Train_Loss: 0.5043733309047029 ; Val_loss:0.7032104789505197 ; percent_acc : 84.0 % \n",
      "Average accuracy acheived: 84.0 %\n"
     ]
    }
   ],
   "source": [
    "# Size of the input \n",
    "input_dim = x_train.shape[1]\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.03\n",
    "nHiddens = 128\n",
    "nLabels = 10\n",
    "\n",
    "# Create neural network\n",
    "net = NN()\n",
    "net.add_layer(Linear(input_dim, nHiddens))\n",
    "net.add_layer(ReLU())\n",
    "net.add_layer(Linear(nHiddens, nLabels))\n",
    "\n",
    "trained_net = train(net, x_train , y_train, x_test, y_test, minibatch_size=420, \n",
    "           nEpochs=100, learning_rate=learning_rate )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R8McYIJwJrXL"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "88vtTNZGzd5_"
   },
   "source": [
    "#### fprop a single image and showing its prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "WBMJrew2zd6B",
    "outputId": "1d81c43a-7711-4a1e-a98f-f76cf6d6abda"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAI0AAACOCAYAAAAMyosLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0DDKsNAwyrDQMMqw0DDKsNAwyvwf9QnKKjqXXLcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: [6]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "i = 130\n",
    "img = x_test[i].reshape(32, 32)\n",
    "figure = plt.figure(figsize=(2, 2))\n",
    "plt.imshow(img, cmap=\"Blues_r\")\n",
    "plt.show()\n",
    "predicted_label = trained_net.predict(x_test[i])\n",
    "print(\"Predicted Label: {}\".format(predicted_label))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AnupriyaRamachandran-DLCP-Project1-Milestone2.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
